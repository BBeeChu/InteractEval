{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle \n",
    "from src.gpt_model import Model\n",
    "import ast\n",
    "\n",
    "DIMENSION = \"coherence\"\n",
    "coherence_rubric = 'Coherence - the collective quality of all sentences. We align this dimension with the DUC quality question of structure and coherence whereby \"the summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to a coherent body of information about a topic.\"'\n",
    "consistency_rubric = \"Consistency - the factual alignment between the summary and the summarized source. A factually consistent summary contains only statements that are entailed by the source document. Annotators were also asked to penalize summaries that contained hallucinated facts. \"\n",
    "fluency_rubric = 'Fluency - the quality of individual sentences. Drawing again from the DUC quality guidelines, sentences in the summary \"should have no formatting problems, capitalization errors or obviously ungrammatical sentences (e.g., fragments, missing components) that make the text difficult to read.\"'\n",
    "relevance_rubric = \"Relevance - selection of important content from the source. The summary should include only important information from the source document. Annotators were instructed to penalize summaries which contained redundancies and excess information.\"\n",
    "\n",
    "DIMENSION_RUBRIC = {\n",
    "    \"coherence\": coherence_rubric,\n",
    "    \"consistency\": consistency_rubric, \n",
    "    \"fluency\": fluency_rubric,\n",
    "    \"relevance\": relevance_rubric\n",
    "}\n",
    "\n",
    "with open(f\"./data/{DIMENSION}/human_llm_attributes.txt\", \"r\") as file:\n",
    "    attributes_list = file.read().splitlines()\n",
    "attributes = \"\\n\".join(attributes_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4\n"
     ]
    }
   ],
   "source": [
    "with open(\"./api_keys.json\", \"r\") as file:\n",
    "    api_keys = json.load(file)\n",
    "\n",
    "OPENAI_API_KEY = api_keys[\"openai\"]\n",
    "\n",
    "gpt4 = Model(model=\"gpt-4\", temperature=0.0, api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./prompts/checklist_construction/component_extraction/system_prompt.txt\", \"r\") as file:\n",
    "    sys_prompt = file.read()\n",
    "\n",
    "with open(\"./prompts/checklist_construction/component_extraction/user_prompt.txt\", \"r\") as file:\n",
    "    user_prompt = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [\n",
    "    {\"role\":\"system\", \"content\": sys_prompt.format(DIMENSION, DIMENSION, DIMENSION_RUBRIC[DIMENSION])},\n",
    "    {\"role\": \"user\", \"content\": user_prompt.format(attributes)}\n",
    "]\n",
    "\n",
    "gpt4_response = gpt4.ask_chatgpt(prompt_list)\n",
    "components = ast.literal_eval(gpt4_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attributes Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./prompts/checklist_construction/attributes_clustering/system_prompt.txt\", \"r\") as file:\n",
    "    sys_prompt = file.read()\n",
    "\n",
    "with open(\"./prompts/checklist_construction/attributes_clustering/user_prompt.txt\", \"r\") as file:\n",
    "    user_prompt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [\n",
    "    {\"role\":\"system\", \"content\": sys_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt.format(components, attributes)}\n",
    "]\n",
    "\n",
    "gpt4_response = gpt4.ask_chatgpt(prompt_list)\n",
    "components_attributes_dic = eval(gpt4_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_attributes = \"\"\n",
    "for k, v in components_attributes_dic.items():\n",
    "    components_attributes += f\"{k}:\\n{v}\\n\\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./prompts/checklist_construction/question_generation/system_prompt.txt\", \"r\") as file:\n",
    "    sys_prompt = file.read()\n",
    "with open(\"./prompts/checklist_construction/question_generation/user_prompt.txt\", \"r\") as file:\n",
    "    user_prompt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [\n",
    "    {\"role\":\"system\", \"content\": sys_prompt.format(DIMENSION)},\n",
    "    {\"role\": \"user\", \"content\": user_prompt.format(DIMENSION, DIMENSION, DIMENSION_RUBRIC[DIMENSION], components_attributes)}\n",
    "]\n",
    "\n",
    "generated_key_questions = gpt4.ask_chatgpt(prompt_list)\n",
    "\n",
    "generated_key_questions = eval(generated_key_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_questions = \"\"\n",
    "for component, question in generated_key_questions.items():\n",
    "    key_questions += \"- \"+component+\": \"+question+\"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub-question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./prompts/checklist_construction/sub_question_generation/system_prompt.txt\", \"r\") as file:\n",
    "    sys_prompt = file.read()\n",
    "with open(\"./prompts/checklist_construction/sub_question_generation/user_prompt.txt\", \"r\") as file:    \n",
    "    user_prompt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [\n",
    "    {\"role\":\"system\", \"content\": sys_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt.format(DIMENSION, DIMENSION, DIMENSION, DIMENSION_RUBRIC[DIMENSION], key_questions)}\n",
    "]\n",
    "generated_sub_questions = gpt4.ask_chatgpt(prompt_list)\n",
    "generated_sub_questions = eval(generated_sub_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_questions = \"\"\n",
    "for component, sub_question_list in generated_sub_questions.items():\n",
    "    for sub_question in sub_question_list:\n",
    "        sub_questions += f\"- {sub_question}\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./prompts/checklist_construction/question_validation/system_prompt.txt\", \"r\") as file:\n",
    "    sys_prompt = file.read()\n",
    "with open(\"./prompts/checklist_construction/question_validation/user_prompt.txt\", \"r\") as file:    \n",
    "    user_prompt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [\n",
    "    {\"role\":\"system\", \"content\": sys_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt.format(DIMENSION, DIMENSION, DIMENSION_RUBRIC[DIMENSION], DIMENSION, sub_questions)}\n",
    "]\n",
    "\n",
    "final_sub_questions = gpt4.ask_chatgpt(prompt_list)\n",
    "\n",
    "final_sub_questions_list = ast.literal_eval(final_sub_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist = \"\"\n",
    "\n",
    "for sub_question in final_sub_questions_list:\n",
    "\n",
    "    checklist+=f\"- {sub_question}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Does the summary start with an introduction that sets the context?\n",
      "- Does the summary have a body that elaborates on the main points?\n",
      "- Does the summary conclude with a summary or wrap-up of the information presented?\n",
      "- Does each sentence in the summary follow logically from the previous one?\n",
      "- Are there effective transitions between ideas in the summary?\n",
      "- Does the summary avoid abrupt shifts in topic or context?\n",
      "- Does the summary clearly present the main points of the original document?\n",
      "- Does the summary avoid including unnecessary details?\n",
      "- Is the language used in the summary straightforward and easy to understand?\n",
      "- Is the tone and style of the summary consistent throughout?\n",
      "- Does the summary include all key details from the original document?\n",
      "- Does the summary conclude logically, without leaving out essential information?\n",
      "- Does the summary avoid ending abruptly or leaving the reader with unanswered questions?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(checklist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
