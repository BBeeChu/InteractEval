{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import google.generativeai as genai\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "with open(\"../data/summeval.json\") as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../api_keys.json\", \"r\") as file:\n",
    "    api_keys = json.load(file)\n",
    "GOOGLE_API_KEY = api_keys[\"google\"]\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-pro-latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSION = \"coherence\"\n",
    "DIMENSION_RUBRIC = {\n",
    "    \"coherence\":'Coherence ( 1 - 5 ) - the collective quality of all sentences. We align this dimension with the DUC quality question of structure and coherence whereby \"the summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to a coherent body of information about a topic.\"',\n",
    "    \"consistency\":\"Consistency ( 1 - 5 ) - the factual alignment between the summary and the summarized source. A factually consistent summary contains only statements that are entailed by the source document. Annotators were also asked to penalize summaries that contained hallucinated facts.\",\n",
    "    \"fluency\":'Fluency ( 1 - 5 ) - the quality of individual sentences. Drawing again from the DUC quality guidelines, sentences in the summary \"should have no formatting problems, capitalization errors or obviously ungrammatical sentences (e.g., fragments, missing components) that make the text difficult to read.\"',\n",
    "    \"relevance\":\"Relevance ( 1 - 5 ) - selection of important content from the source. The summary should include only important information from the source document. Annotators were instructed to penalize summaries which contained redundancies and excess information.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list = pd.read_pickle(f\"../data/{DIMENSION}/sample.pkl\")\n",
    "\n",
    "sampled_source = list()\n",
    "sampled_summary = list()\n",
    "sampled_score = list()\n",
    "\n",
    "for sample in sample_list:\n",
    "    sampled_source.append(sample[\"source\"])\n",
    "    sampled_summary.append(sample[\"system_output\"])\n",
    "    sampled_score.append(sample[\"scores\"][DIMENSION])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../prompts/think_aloud/gemini/user_prompt.txt\", \"r\") as file:\n",
    "    user_prompt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_response = model.generate_content(user_prompt.format(DIMENSION, DIMENSION, DIMENSION, DIMENSION,\n",
    "                                        DIMENSION, DIMENSION_RUBRIC[DIMENSION], sampled_source[0], sampled_summary[0], sampled_score[0],\n",
    "                                        sampled_source[1], sampled_summary[1], sampled_score[1], sampled_source[2], sampled_summary[2], sampled_score[2],\n",
    "                                        sampled_source[3], sampled_summary[3], sampled_score[3]))\n",
    "\n",
    "gemini_response = ''.join(re.findall(r'\\{[^}]*\\}', gemini_response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': \"Does the summary read like a single, cohesive paragraph rather than a collection of disjointed sentences?  The rubric emphasizes a 'well-structured and well-organized' summary, indicating a need for clear connections between sentences.\",\n",
       " '2': 'Are there clear transitions between sentences, or do they abruptly shift topics?  The flow from one sentence to the next is crucial for maintaining coherence.',\n",
       " '3': \"Does each sentence logically follow from the one preceding it? The rubric emphasizes a summary that 'builds from sentence to sentence,' implying a logical progression of information.\",\n",
       " '4': 'Does the summary maintain a consistent focus, or does it meander between unrelated aspects of the article? A coherent summary will maintain a clear focus throughout.',\n",
       " '5': 'Could the order of sentences be rearranged without affecting the clarity or flow of information? If so, this suggests a lack of inherent structure and coherence.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(gemini_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
